Project 2: Neural Networks for Indentifying Handwritten Digits

Student1: Mohammed Ibrahim
Student2: Oluwatobi Oni-Orisan


===========PART 2==================
We modified the function that checked the values in the predicted set. If the value of every slot except the correct slot was less than 0.3 and the value in the correct slot was greater than 0.7 we declared that to be a "match". Else, we counted it as an error.

===========PART 3==================

Observations:
    - Learning Rate: 0.002
        - Number of Tests: 5000
        - Total Probable Errors: 5000
        - Test Loss: 0.2632
        - Validation Set Loss: 0.2696
    - Learning Rate: 0.01
        - Number of Tests: 5000
        - Total Probable Errors: 5000
        - Test Loss: 0.09939
        - Validation Set Loss: 0.0996
    - Learning Rate: 0.05
        - Number of Tests: 5000
        - Total probable errors: 4994
        - Test Loss: 0.0868
        - Validation Set Loss: 0.0884
    - Learning Rate: 0.2
        - Number of Tests: 5000
        - Total probable errors: 4929
        - Test Loss: 0.0752
        - Validation Set Loss: 0.0804
    - Learning Rate: 1.0
        - Number of Tests: 5000
        - Total probable errors: 3290
        - Test Loss: 0.0372
        - Validation Set Loss: 0.0498
    - Learning Rate: 5.0
        - Number of Tests  5000
        - Total probable errors  1814 
        - Test Loss: 0.0078
        - Validation Set Loss: 0.0355
    - Learning Rate: 20.0
        - Number of Tests  5000
        - Total probable errors  1505
        - Test Loss: 0.0039
        - Validation Set Loss: 0.0322


===========PART 4(Momentum)==================

Observations :
    - Learning Rate: 0.002
        - Number of Tests: 5000
        - Total Probable Errors: 5000
        - Test Loss: 0.090634853
        - Validation Set Loss: 0.0924245
    - Learning Rate: 0.01
        - Number of Tests: 5000
        - Total Probable Errors: 4952
        - Test Loss: 0.082565799
        - Validation Set Loss: 0.084867187
    - Learning Rate: 0.05
        - Number of Tests: 5000
        - Total Probable Errors: 4209
        - Test Loss: 0.04762511
        - Validation Set Loss: 0.060104921
    - Learning Rate: 0.2
        - Number of Tests: 5000
        - Total Probable Errors: 2295
        - Test Loss: 0.019010777
        - Validation Set Loss: 0.039789379
    - Learning Rate: 1.0
        - Number of Tests: 5000
        - Total Probable Errors: 2019
        - Test Loss: 0.0086130574
        - Validation Set Loss: 0.036413942
    - Learning Rate: 5.0
        - Number of Tests: 5000
        - Total Probable Errors: 2308
        - Test Loss: 0.029293643
        - Validation Set Loss: 0.050930604
    - Learning Rate: 20.0
        - Number of Tests: 5000
        - Total Probable Errors: 4564
        - Test Loss: 0.092157245
        - Validation Set Loss: 0.092216231

Comments: We tweaked the learning rate (tried values greater than 1 and less than 2 and also between 2 and 3), the momentum (tried 8.0, 1.0 and 1.5), and the number of hidden layers (tried 100) and we concluded that the best learning rate we found was 1.0 (with the lowest error rate) 

Fine Tuning: We kept testing our learning rate and we arrived at a final value of 1.4 with 10 hidden units and 0.94 momentum


===========PART 4(Generalization)==================
We added L2 regularization using the best parameters we had (10 hidden layers, learning rate of 1.4 and momentum of 0.94) and it did help. The number of errors, and loss were decresed. We were initially using out own definition of l2 regularization, but when we used the built-in function for regularization (tf.nn.l2_loss()), the errors and loss were decreased even further.

Without regularization, we got overfitting with at 30 hidden layers. There was a large deviation between the loss of the test set and that of the validation set.

Without regularization, we found the best hidden layer to be 10 which is the amount we were given from the start. Upon numerous test values (5,10, 15, 20, 25, 30, 35, 40,.....75) we discovered the best value to be to be 10.

With regularization we found the best hidden layer to be 17. We testes a bunch of values (10, 12, 15, 17, 38, 42, 47, 55) and 17 gave the best result with least amount of deviation between the loss of test and training sets, even when we tweaked the learning rate.

===========PART 4(Generalization)==================

Settings:
   - Hidden Layers: 17
   - Learning Rate: 1.4
Test Data Classification Error: 0.00932
